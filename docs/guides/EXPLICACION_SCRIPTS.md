# ExplicaciÃ³n de los Scripts del Pipeline

DocumentaciÃ³n detallada de quÃ© hace cada script ejecutado en los terminales del pipeline.

---

## ğŸ“‹ Ãndice

1. [Scripts de PreparaciÃ³n](#scripts-de-preparaciÃ³n)
2. [Scripts del Pipeline Principal](#scripts-del-pipeline-principal)
3. [Terminal 6: Enriquecimiento](#terminal-6-enriquecimiento-de-datos-06_data_enrichmentsh)
4. [Scripts de Persistencia](#scripts-de-persistencia)
5. [Scripts de VerificaciÃ³n](#scripts-de-verificaciÃ³n)

---

## ğŸ”§ Scripts de PreparaciÃ³n

### `00_start_hdfs.sh`

**UbicaciÃ³n:** `scripts/run/00_start_hdfs.sh`

**QuÃ© hace:**
- Arranca los servicios de HDFS necesarios para el pipeline
- Inicia el NameNode (gestor del sistema de archivos distribuido)
- Inicia el DataNode (almacenamiento de datos)
- Sale del modo seguro de HDFS para permitir operaciones

**Por quÃ© es necesario:**
- HDFS es el sistema de almacenamiento donde Spark guarda:
  - Checkpoints (para recuperaciÃ³n ante fallos)
  - Resultados procesados (tablas Parquet)
  - Datos maestros (rutas, vehÃ­culos)
- Sin HDFS corriendo, los scripts de Spark fallan con "Connection refused"

**CuÃ¡ndo ejecutarlo:**
- **Siempre primero**, antes de ejecutar cualquier script de Spark
- Solo una vez al inicio de la sesiÃ³n

**Comando:**
```bash
bash scripts/run/00_start_hdfs.sh
```

---

### `01_generate_data.sh`

**UbicaciÃ³n:** `scripts/run/01_generate_data.sh`

**QuÃ© hace:**
- Genera datos de prueba simulando eventos de transporte
- Crea mensajes JSON con informaciÃ³n de vehÃ­culos:
  - ID del vehÃ­culo
  - Timestamp (fecha y hora)
  - Coordenadas GPS (latitud, longitud)
  - Velocidad
  - ID de ruta
  - Estado (IN_TRANSIT, LOADING, MAINTENANCE, etc.)
- EnvÃ­a estos mensajes al topic `raw-data` de Kafka

**Por quÃ© es necesario:**
- El pipeline necesita datos de entrada para procesar
- Simula datos reales de sensores GPS en vehÃ­culos
- Permite probar el pipeline completo sin datos reales

**CuÃ¡ndo ejecutarlo:**
- Al inicio, antes de ejecutar el pipeline
- Puede ejecutarse mÃºltiples veces para generar mÃ¡s datos
- Opcional: puede omitirse si ya hay datos en `raw-data`

**Comando:**
```bash
bash scripts/run/01_generate_data.sh
```

**Ejemplo de datos generados:**
```json
{
  "vehicle_id": "V001",
  "timestamp": "2026-02-11 16:30:00",
  "latitude": 40.7128,
  "longitude": -74.0060,
  "speed": 45.5,
  "route_id": "R001",
  "status": "IN_TRANSIT"
}
```

---

## ğŸ”„ Scripts del Pipeline Principal

### `02_data_cleaning.sh` (Terminal 2)

**UbicaciÃ³n:** `scripts/run/02_data_cleaning.sh`  
**Script Python:** `processing/spark/sql/data_cleaning.py`

**QuÃ© hace:**
1. **Lee datos desde Kafka** (`raw-data` topic)
   - Consume mensajes JSON en tiempo real usando Spark Structured Streaming
   - Procesa datos en micro-batches cada 10 segundos

2. **Limpia y valida los datos:**
   - Normaliza formatos (trim, uppercase)
   - Valida coordenadas GPS (latitud entre -90 y 90, longitud entre -180 y 180)
   - Valida velocidad (no negativa, no excesiva)
   - Maneja valores nulos
   - Convierte timestamps a formato estÃ¡ndar

3. **Elimina duplicados:**
   - Usa `dropDuplicates` basado en `vehicle_id` + `timestamp`
   - Evita procesar el mismo evento mÃºltiples veces

4. **AÃ±ade metadatos:**
   - Campo `cleaned_at`: timestamp de cuÃ¡ndo se limpiÃ³ el dato
   - Mantiene todos los campos originales vÃ¡lidos

5. **Escribe resultados:**
   - En Kafka topic `filtered-data` (datos limpios para siguiente etapa)
   - Guarda checkpoint en HDFS para recuperaciÃ³n

**Por quÃ© es necesario:**
- Los datos en `raw-data` pueden tener errores, duplicados o formatos inconsistentes
- Esta etapa asegura calidad de datos antes del anÃ¡lisis
- Es el primer paso del procesamiento en el pipeline

**CuÃ¡ndo ejecutarlo:**
- DespuÃ©s de generar datos (`01_generate_data.sh`)
- Debe estar corriendo continuamente mientras hay datos para procesar
- Se ejecuta en **Terminal 2**

**Comando:**
```bash
bash scripts/run/02_data_cleaning.sh
```

**Salidas:**
- **Kafka:** Topic `filtered-data` con datos limpios
- **HDFS:** Checkpoint en `/user/hadoop/checkpoints/cleaning/`

**Ejemplo de datos de salida:**
```json
{
  "vehicle_id": "V001",
  "timestamp": "2026-02-11T16:30:00.000+01:00",
  "latitude": 40.7128,
  "longitude": -74.0060,
  "speed": 45.5,
  "route_id": "R001",
  "status": "IN_TRANSIT",
  "cleaned_at": "2026-02-11T17:04:30.038+01:00"
}
```

---

### `03_delay_analysis.sh` (Terminal 3)

**UbicaciÃ³n:** `scripts/run/03_delay_analysis.sh`  
**Script Python:** `processing/spark/streaming/delay_analysis.py`

**QuÃ© hace:**
1. **Lee datos limpios desde Kafka** (`filtered-data` topic)
   - Consume datos ya validados y normalizados
   - Procesa en micro-batches cada 10 segundos

2. **Calcula retrasos:**
   - Determina si un vehÃ­culo estÃ¡ en retraso usando heurÃ­stica:
     - `is_delayed = True` si velocidad < 10 km/h
     - `is_delayed = False` si velocidad >= 10 km/h
   - Esta es una aproximaciÃ³n simple (en producciÃ³n usarÃ­a tiempos estimados vs reales)

3. **Agrupa por ventanas de tiempo:**
   - Crea ventanas de **15 minutos** usando `window` function
   - Agrupa por `route_id` y ventana de tiempo
   - Calcula agregados para cada ventana:
     - `total_vehicles`: nÃºmero de vehÃ­culos en esa ventana/ruta
     - `avg_speed`: velocidad media
     - `delayed_count`: nÃºmero de vehÃ­culos en retraso
     - `delay_percentage`: porcentaje de vehÃ­culos en retraso
     - `max_speed`, `min_speed`: velocidades mÃ¡xima y mÃ­nima

4. **Escribe resultados en mÃºltiples destinos:**
   - **Kafka topic `alerts`:** Agregados en formato JSON para servicios externos
   - **HDFS Parquet:** Tabla `delay_aggregates` particionada por `route_id`
   - **Consola:** Para debugging y monitoreo

**Por quÃ© es necesario:**
- Proporciona mÃ©tricas agregadas de retrasos por ruta y tiempo
- Permite identificar rutas problemÃ¡ticas
- Genera alertas en tiempo real para sistemas externos
- Almacena resultados histÃ³ricos en HDFS para anÃ¡lisis posterior

**CuÃ¡ndo ejecutarlo:**
- DespuÃ©s de que `data_cleaning.py` haya procesado algunos datos
- Debe estar corriendo continuamente
- Se ejecuta en **Terminal 3**

**Comando:**
```bash
bash scripts/run/03_delay_analysis.sh
```

**Salidas:**
- **Kafka:** Topic `alerts` con agregados JSON
- **HDFS:** Tabla `delay_aggregates` en `/user/hive/warehouse/delay_aggregates/`
- **Consola:** Logs de cada batch procesado

**Ejemplo de mensaje en `alerts`:**
```json
{
  "window_start": "2026-02-11T16:45:00.000+01:00",
  "window_end": "2026-02-11T17:00:00.000+01:00",
  "route_id": "R003",
  "total_vehicles": 3,
  "avg_speed": 8.5,
  "delayed_count": 2,
  "delay_percentage": 66.7,
  "max_speed": 15.2,
  "min_speed": 0.0,
  "analysis_timestamp": "2026-02-11T17:05:30.089+01:00"
}
```

**InterpretaciÃ³n:**
- En la ventana 16:45-17:00, ruta R003
- 3 vehÃ­culos registrados
- Velocidad media: 8.5 km/h (muy baja)
- 2 de 3 vehÃ­culos en retraso (66.7%)
- Indica posible problema en esa ruta

---

## ğŸ’¾ Scripts de Persistencia

### `04_mongodb_consumer.sh` (Terminal 4)

**UbicaciÃ³n:** `scripts/run/04_mongodb_consumer.sh`  
**Script Python:** `storage/mongodb/kafka_to_mongodb_alerts.py`

**QuÃ© hace:**
1. **Conecta a Kafka y MongoDB:**
   - Se conecta al topic `alerts` de Kafka
   - Se conecta a MongoDB en `127.0.0.1:27017`
   - Usa base de datos `transport_db`

2. **Consume mensajes del topic `alerts`:**
   - Lee agregados de retrasos generados por `delay_analysis.py`
   - Procesa mensajes JSON en tiempo real

3. **Parsea y transforma datos:**
   - Convierte timestamps ISO a objetos `datetime` de MongoDB
   - Valida estructura de los mensajes

4. **Inserta/actualiza en MongoDB:**
   - ColecciÃ³n: `route_delay_aggregates`
   - Usa `upsert` (inserta si no existe, actualiza si existe)
   - Clave Ãºnica: `route_id` + `window_start`
   - Crea Ã­ndices automÃ¡ticamente para consultas rÃ¡pidas

5. **Muestra progreso:**
   - Imprime cada documento insertado/actualizado
   - Muestra estadÃ­sticas: ruta, ventana, vehÃ­culos, porcentaje de retraso

**Por quÃ© es necesario:**
- MongoDB permite consultas rÃ¡pidas de agregados de retrasos
- Ideal para dashboards y aplicaciones web
- Complementa el almacenamiento en HDFS (mÃ¡s lento pero mÃ¡s escalable)
- Permite alertas en tiempo real

**CuÃ¡ndo ejecutarlo:**
- DespuÃ©s de que `delay_analysis.py` haya generado algunos alerts
- Debe estar corriendo continuamente para procesar nuevos alerts
- Se ejecuta en **Terminal 4**

**Comando:**
```bash
bash scripts/run/04_mongodb_consumer.sh
```

**Salidas:**
- **MongoDB:** ColecciÃ³n `route_delay_aggregates` con documentos JSON
- **Consola:** Mensajes de inserciÃ³n/actualizaciÃ³n

**Ejemplo de salida en consola:**
```
âœ“ Insertado: R003 @ 2026-02-11 17:00:00+01:00 (2 vehÃ­culos, 100.0% retraso)
âœ“ Insertado: R002 @ 2026-02-11 16:45:00+01:00 (4 vehÃ­culos, 100.0% retraso)
â†» Actualizado: R003 @ 2026-02-11 16:45:00+01:00
```

**Estructura del documento en MongoDB:**
```json
{
  "_id": ObjectId("..."),
  "route_id": "R003",
  "window_start": ISODate("2026-02-11T17:00:00.000Z"),
  "window_end": ISODate("2026-02-11T17:15:00.000Z"),
  "total_vehicles": 2,
  "avg_speed": 5.2,
  "delayed_count": 2,
  "delay_percentage": 100.0,
  "max_speed": 8.1,
  "min_speed": 0.0,
  "analysis_timestamp": ISODate("2026-02-11T17:05:30.089Z")
}
```

---

## âœ… Scripts de VerificaciÃ³n

### `05_verify_mongodb.sh`

**UbicaciÃ³n:** `scripts/run/05_verify_mongodb.sh`  
**Script Python:** `storage/mongodb/verify_data.py`

**QuÃ© hace:**
1. **Conecta a MongoDB:**
   - Se conecta a `transport_db` en MongoDB
   - Verifica las tres colecciones principales

2. **Cuenta documentos:**
   - `route_delay_aggregates`: Agregados de retrasos
   - `vehicle_status`: Estado de vehÃ­culos (si existe)
   - `bottlenecks`: Cuellos de botella (si existe)

3. **Muestra estadÃ­sticas:**
   - Ãšltimos documentos insertados
   - Promedio de retraso por ruta
   - NÃºmero de ventanas procesadas por ruta

4. **Ordena resultados:**
   - Por porcentaje de retraso (descendente)
   - Por fecha (mÃ¡s recientes primero)

**Por quÃ© es necesario:**
- Permite verificar que el pipeline estÃ¡ funcionando
- Muestra estadÃ­sticas Ãºtiles para anÃ¡lisis
- Ãštil para debugging y monitoreo

**CuÃ¡ndo ejecutarlo:**
- En cualquier momento para verificar el estado
- Puede ejecutarse mÃºltiples veces
- No necesita estar corriendo continuamente

**Comando:**
```bash
bash scripts/run/05_verify_mongodb.sh
```

**CaracterÃ­sticas de la salida (quÃ© significa cada nÃºmero, Ãºltimos 5, promedios, vehÃ­culos en retraso, bottlenecks):** Ver **docs/guides/VERIFICACION_MONGODB_CARACTERISTICAS.md**.

**Ejemplo de salida:**
```
=== VerificaciÃ³n de datos en MongoDB ===

ğŸ“Š route_delay_aggregates: 26 documentos

Ãšltimos 5 documentos:
  - Ruta: R006, Ventana: 2026-02-11 16:00:00, Retraso: 100.0%, VehÃ­culos: 1
  - Ruta: R005, Ventana: 2026-02-11 16:00:00, Retraso: 100.0%, VehÃ­culos: 1
  ...

Promedio de retraso por ruta:
  - R002: 100.0% (ventanas: 1)
  - R004: 100.0% (ventanas: 4)
  - R003: 91.7% (ventanas: 4)
  ...
```

---

## ğŸ”„ Flujo Completo del Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PREPARACIÃ“N                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€ 00_start_hdfs.sh â†’ Arranca HDFS
         â”‚
         â””â”€ 01_generate_data.sh â†’ Genera datos de prueba
                                  â†“
                            raw-data (Kafka)
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PROCESAMIENTO (Streaming)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€ Terminal 2: 02_data_cleaning.sh
         â”‚   â†“ Lee raw-data
         â”‚   â†“ Limpia y valida
         â”‚   â†“ Escribe filtered-data (Kafka)
         â”‚
         â”œâ”€ Terminal 3: 03_delay_analysis.sh
         â”‚   â†“ Lee filtered-data
         â”‚   â†“ Calcula agregados (ventanas 15 min)
         â”‚   â†“ Escribe alerts (Kafka) + HDFS
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PERSISTENCIA                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â””â”€ Terminal 4: 04_mongodb_consumer.sh
             â†“ Lee alerts (Kafka)
             â†“ Inserta en MongoDB
             â†“ route_delay_aggregates
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  VERIFICACIÃ“N                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â””â”€ 05_verify_mongodb.sh â†’ Muestra estadÃ­sticas
```

---

### Terminal 6: Enriquecimiento de datos (`06_data_enrichment.sh`)

**Script Python:** `processing/spark/sql/data_enrichment.py`

**QuÃ© hace:** Lee el stream de `filtered-data` (Kafka), lo cruza con las tablas maestras `master_routes` y `master_vehicles` (HDFS), aÃ±ade campos derivados (`is_delayed`, `enriched_at`) y escribe el resultado en HDFS en formato Parquet, particionado por `route_id` y `partition_date`.

**Para quÃ© sirve:** Tener eventos de transporte enriquecidos con nombre de ruta, origen/destino, tipo de vehÃ­culo y empresa, listos para anÃ¡lisis o reporting en HDFS.

**Salida en HDFS:** `hdfs://localhost:9000/user/hadoop/processed/enriched/` con carpetas `route_id=R001`, `route_id=R002`, etc., y dentro `partition_date=YYYY-MM-DD` con ficheros `.parquet`.

**DocumentaciÃ³n detallada** (ejemplos de salida, explicaciÃ³n de cada elemento, comandos de verificaciÃ³n): **docs/guides/ENRIQUECIMIENTO_DATOS.md**.

---

## ğŸ“Š Resumen por Terminal

| Terminal | Script | QuÃ© Hace | Entrada | Salida |
|----------|--------|----------|---------|--------|
| **0** | `00_start_hdfs.sh` | Arranca HDFS | - | HDFS activo |
| **1** | `01_generate_data.sh` | Genera datos | - | `raw-data` (Kafka) |
| **2** | `02_data_cleaning.sh` | Limpia datos | `raw-data` | `filtered-data` (Kafka) |
| **3** | `03_delay_analysis.sh` | Analiza retrasos | `filtered-data` | `alerts` (Kafka) + HDFS |
| **4** | `04_mongodb_consumer.sh` | Persiste en MongoDB | `alerts` | MongoDB |
| **5** | `05_verify_mongodb.sh` | Verifica datos | MongoDB | EstadÃ­sticas (consola) |
| **6** | `06_data_enrichment.sh` | Enriquecimiento (stream + maestros) | `filtered-data` + HDFS master_* | HDFS `processed/enriched` |

---

## ğŸ¯ Conceptos Clave

### Spark Structured Streaming
- Procesa datos en tiempo real usando micro-batches
- Permite recuperaciÃ³n ante fallos mediante checkpoints
- Procesa datos cada 10 segundos (configurable)

### Ventanas de Tiempo (Windows)
- Agrupa eventos por intervalos de tiempo (15 minutos)
- Permite anÃ¡lisis temporal de tendencias
- Ãštil para detectar patrones en el tiempo

### Checkpoints
- Guardan el estado del procesamiento en HDFS
- Permiten recuperar desde el Ãºltimo punto procesado
- Evitan perder datos ante fallos

### Upsert en MongoDB
- Inserta si no existe, actualiza si existe
- Evita duplicados usando clave Ãºnica
- Mantiene datos actualizados automÃ¡ticamente

---

## ğŸ“ Notas Importantes

1. **Orden de ejecuciÃ³n:** Los scripts deben ejecutarse en orden (0 â†’ 1 â†’ 2 â†’ 3 â†’ 4)
2. **Tiempo de espera:** Entre cada terminal, esperar ~30 segundos para que procese datos
3. **EjecuciÃ³n continua:** Terminales 2, 3 y 4 deben seguir corriendo
4. **HDFS primero:** Siempre arrancar HDFS antes de ejecutar scripts de Spark
5. **Datos persistentes:** Los datos en Kafka y HDFS persisten aunque los scripts se detengan

---

## ğŸ” Troubleshooting

**Si un script falla:**
1. Verificar que HDFS estÃ¡ corriendo (`jps | grep NameNode`)
2. Verificar que Kafka estÃ¡ corriendo (`jps | grep Kafka`)
3. Verificar que MongoDB estÃ¡ corriendo (`pgrep mongod`)
4. Revisar logs del script para ver el error especÃ­fico

**Si no hay datos:**
1. Verificar que hay datos en el topic anterior (`raw-data` â†’ `filtered-data` â†’ `alerts`)
2. Esperar unos segundos (los batches se procesan cada 10 segundos)
3. Generar mÃ¡s datos si es necesario (`01_generate_data.sh`)
