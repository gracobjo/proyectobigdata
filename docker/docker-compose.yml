# Proyecto Big Data - Stack completo para pruebas (Docker)
# Uso: cd docker && docker compose up -d
# Perfil light (Railway / recursos limitados): docker compose --profile light up -d

services:
  # --- HADOOP (HDFS + YARN) - perfil full ---
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    hostname: namenode
    restart: unless-stopped
    ports:
      - "9870:9870"   # Web UI
      - "9000:9000"   # RPC HDFS
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    env_file:
      - ./hadoop.env
    profiles:
      - full

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    hostname: datanode
    restart: unless-stopped
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./hadoop.env
    profiles:
      - full

  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    container_name: resourcemanager
    hostname: resourcemanager
    restart: unless-stopped
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864"
    env_file:
      - ./hadoop.env
    profiles:
      - full

  nodemanager:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: nodemanager
    hostname: nodemanager
    restart: unless-stopped
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
    env_file:
      - ./hadoop.env
    profiles:
      - full

  # --- KAFKA (KRaft, sin Zookeeper) ---
  kafka:
    image: bitnami/kafka:3.7
    container_name: kafka
    hostname: kafka
    restart: unless-stopped
    ports:
      - "9092:9092"
    environment:
      KAFKA_CFG_NODE_ID: "0"
      KAFKA_CFG_PROCESS_ROLES: "controller,broker"
      KAFKA_CFG_LISTENERS: "PLAINTEXT://:9092,CONTROLLER://:9093"
      KAFKA_CFG_ADVERTISED_LISTENERS: "PLAINTEXT://kafka:9092"
      KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP: "CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT"
      KAFKA_CFG_CONTROLLER_LISTENER_NAMES: "CONTROLLER"
      KAFKA_CFG_INTER_BROKER_LISTENER_NAME: "PLAINTEXT"
    volumes:
      - kafka_data:/bitnami/kafka
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics.sh --bootstrap-server localhost:9092 --list || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5

  # --- MONGODB ---
  mongodb:
    image: mongo:7
    container_name: mongodb
    hostname: mongodb
    restart: unless-stopped
    ports:
      - "27017:27017"
    environment:
      MONGO_INITDB_DATABASE: transport_db
    volumes:
      - mongodb_data:/data/db
    healthcheck:
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
      interval: 10s
      timeout: 5s
      retries: 5

  # --- NIFI ---
  nifi:
    image: apache/nifi:2.0.0
    container_name: nifi
    hostname: nifi
    restart: unless-stopped
    ports:
      - "8443:8443"
      - "8080:8080"
    environment:
      NIFI_WEB_HTTP_PORT: "8080"
      NIFI_WEB_HTTPS_PORT: "8443"
      NIFI_REMOTE_INPUT_HOST: "nifi"
      NIFI_CLUSTER_IS_NODE: "false"
      NIFI_ZOOKEEPER_CONNECT_STRING: ""
      NIFI_KAFKA_BOOTSTRAP_SERVERS: "kafka:9092"
    volumes:
      - nifi_data:/opt/nifi/nifi-current/database_repository
      - nifi_flow:/opt/nifi/nifi-current/flowfile_repository
      - nifi_content:/opt/nifi/nifi-current/content_repository
      - nifi_provenance:/opt/nifi/nifi-current/provenance_repository
    depends_on:
      kafka:
        condition: service_healthy
    profiles:
      - full

  # --- HIVE METASTORE (Postgres + metastore) - perfil full ---
  hive-metastore-db:
    image: postgres:15-alpine
    container_name: hive-metastore-db
    hostname: hive-metastore-db
    restart: unless-stopped
    environment:
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive
      POSTGRES_DB: metastore
    volumes:
      - hive_metastore_db:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U hive -d metastore"]
      interval: 5s
      timeout: 3s
      retries: 5
    profiles:
      - full

  hive-metastore:
    image: bde2020/hive-metastore-postgresql:2.3.0-postgresql9.6
    container_name: hive-metastore
    hostname: hive-metastore
    restart: unless-stopped
    environment:
      SERVICE_PRECONDITION: "hive-metastore-db:5432"
      DB_DRIVER: postgres
      SERVICE_NAME: metastore
      IS_RESUME: "true"
      DB_HOST: hive-metastore-db
      DB_PORT: "5432"
      DB_USER: hive
      DB_PASSWORD: hive
      DB_DATABASE: metastore
      METASTORE_PORT: "9083"
    ports:
      - "9083:9083"
    depends_on:
      hive-metastore-db:
        condition: service_healthy
    profiles:
      - full

  # --- AIRFLOW (Postgres + init + webserver + scheduler) ---
  airflow-init:
    image: apache/airflow:2.10.0
    container_name: airflow-init
    command: >
      bash -c "airflow db init && airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin || true"
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW_UID: "50000"
    depends_on:
      airflow-db:
        condition: service_healthy
    user: "50000:0"
    profiles:
      - full

  airflow-db:
    image: postgres:15-alpine
    container_name: airflow-db
    hostname: airflow-db
    restart: unless-stopped
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - airflow_db:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow -d airflow"]
      interval: 5s
      timeout: 3s
      retries: 5
    profiles:
      - full

  airflow-webserver:
    image: apache/airflow:2.10.0
    container_name: airflow-webserver
    hostname: airflow-webserver
    restart: unless-stopped
    command: webserver
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW_UID: "50000"
      _PIP_ADDITIONAL_REQUIREMENTS: ""
    ports:
      - "8081:8080"
    volumes:
      - airflow_dags:/opt/airflow/dags
      - ../orchestration/airflow/dags:/opt/airflow/dags:ro
    depends_on:
      airflow-db:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    user: "50000:0"
    profiles:
      - full

  airflow-scheduler:
    image: apache/airflow:2.10.0
    container_name: airflow-scheduler
    hostname: airflow-scheduler
    restart: unless-stopped
    command: scheduler
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW_UID: "50000"
    volumes:
      - airflow_dags:/opt/airflow/dags
      - ../orchestration/airflow/dags:/opt/airflow/dags:ro
    depends_on:
      airflow-db:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
      airflow-webserver:
        condition: service_started
    user: "50000:0"
    profiles:
      - full

  # --- API REST (FastAPI) ---
  api:
    build:
      context: ..
      dockerfile: docker/Dockerfile.api
    container_name: api
    hostname: api
    restart: unless-stopped
    ports:
      - "5000:5000"
    environment:
      MONGO_URI: mongodb://mongodb:27017/
      DB_NAME: transport_db
    depends_on:
      mongodb:
        condition: service_healthy
    profiles:
      - full
      - light

  # --- InicializaciÃ³n: topics Kafka (ejecutar una vez tras levantar kafka) ---
  kafka-init:
    image: bitnami/kafka:3.7
    container_name: kafka-init
    command: >
      bash -c "
        echo 'Esperando Kafka...' && sleep 15 &&
        kafka-topics.sh --create --if-not-exists --bootstrap-server kafka:9092 --topic raw-data --partitions 3 --replication-factor 1 &&
        kafka-topics.sh --create --if-not-exists --bootstrap-server kafka:9092 --topic filtered-data --partitions 3 --replication-factor 1 &&
        kafka-topics.sh --create --if-not-exists --bootstrap-server kafka:9092 --topic alerts --partitions 1 --replication-factor 1 &&
        echo 'Topics listos.'
      "
    depends_on:
      kafka:
        condition: service_healthy
    profiles:
      - full
      - light

volumes:
  hadoop_namenode:
  hadoop_datanode:
  kafka_data:
  mongodb_data:
  nifi_data:
  nifi_flow:
  nifi_content:
  nifi_provenance:
  hive_metastore_db:
  airflow_db:
  airflow_dags:
